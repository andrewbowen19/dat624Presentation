---
title: "DATA 624 Project 2"
author: "Andrew Bowen, Josh Forster, John Cruz"
date: "2024-04-25"
output: html_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(caret)
library(corrplot)
library(mice)
library(ModelMetrics)
library(randomForest)
library(nnet)
```


## Exploratory Data Analysis

First, we'll read in our training and testing datasets. We've converted them via a basic R script and they are available on our [GitHub here](https://github.com/andrewbowen19/data624Presentation/tree/main/data)


```{r read-in}
# Read in training and testing datasets
training <- read.csv("https://raw.githubusercontent.com/andrewbowen19/data624Presentation/main/data/train.csv")
test <- read.csv("https://raw.githubusercontent.com/andrewbowen19/data624Presentation/main/data/test.csv")
```


We can take a brief look at our training data summary.
```{r train-summary}
summary(training)
```


We can take a look at the distribution of our outcome variable (`PH`)
```{r plot-ph-dist}
# Plot pH level distribution: our output variable
ggplot(training, aes(x=PH)) + geom_histogram()
```



### Data Wrangling
```{r wrangling}
# Convert Brand.Code to a factor as it's categorical
training$Brand.Code <- as.factor(training$Brand.Code)
test$Brand.Code <- as.factor(test$Brand.Code)
```


## Imputation
Our dataset is missing values, so we'll have to imput using the `mice` library in R. For our purposes, we'll use [predictive mean matching as our imputation method](https://stefvanbuuren.name/fimd/sec-pmm.html), since we may or may not need to transform our target variable this is a good method as it's robust to transformations of our output variable $Y$.
```{r message=FALSE, warning=FALSE}
# Impute using the mice package
# TODO(abowen): fix imputation issue
imputed <- mice(training, meth="pmm", seed=1234, printFlag = FALSE)
```


```{r}
# Set the training set to be imputed values
train <- complete(imputed)
head(train)
```

## Feature Selection

Next, we'll look at the correlations between our features and output PH

```{r}
correlations <- cor(as.matrix(train %>% dplyr::select(-c("Brand.Code"))))

corrplot(correlations)
```

It's also helpful to see a pairwise distribution plot, which we can do via the `featurePlot` function from the `caret` library


## Modeling


### Multiple Linear Regression
First, we'll fit a basic multiple linear regression model to our data
```{r}
mlmModel <- lm(PH ~ ., train)
```



### Random Forest
We'll train a random forest to predict the `PH` variable as well. Random forests are an ensemble method based on training several decision trees

```{r}
# Train random forest mdoel
set.seed(12345)
rfModel <- randomForest(PH ~ ., data = train)
```


### Neural Network
```{r}
x <- train %>% select(-c(PH, Brand.Code))
y <- train$PH
nnModel <- nnet(x, y, size=16)
```


### XGBoost
One other poopular method is that of XGBoost

```{r}
# Set up parameter grid for XG Boost tuning
param <-  data.frame(nrounds=c(100), max_depth = c(2),eta =c(0.3),gamma=c(0),
                    colsample_bytree=c(0.8),min_child_weight=c(1),subsample=c(1)) 

xgBoostModel <- train(PH~., data=train, method="xgbTree",
      trControl=trainControl(method="none"), tuneGrid=param)
```



## Model Evalutation


We'll need to impute our test data
```{r impute-test-data}
testImputed <- mice(test, method="pmm", seed=1234, printFlag = FALSE)

test <- complete(testImputed) %>% select(-c(PH, Brand.Code))
```



```{r predict}
# Predict using all trained models
predictionsLinear <- predict(mlmModel, test)
predictionsRandomForest <- predict(rfModel, test)
predictionsNN <- predict.nnet(nnModel, test)
predictionsXGBoost <- predict(xgBoostModel, test)
```



Lastly, we'll combine our prediction columns into 
```{r combine-predictions}
predictions <- as.data.frame(cbind(linear = predictionsLinear,xgBoost = predictionsXGBoost, randomForest = predictionsRandomForest))#, neuralNet = predictionsNN)

# Replace with local path if needed
write_csv(predictions, "/Users/andrewbowen/CUNY/data624Presentation/data/project2-predictions.csv")
```

