---
title: "DATA 624 Project 2"
author: "Andrew Bowen, Josh Forster, John Cruz"
date: "2024-04-25"
output: html_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(caret)
library(corrplot)
library(mice)
library(ModelMetrics)
library(randomForest)
library(nnet)
library(MASS)
```


## Exploratory Data Analysis

First, we'll read in our training and testing datasets. We've converted them via a basic R script and they are available on our [GitHub here](https://github.com/andrewbowen19/data624Presentation/tree/main/data).


```{r read-in}
# Read in training and testing datasets
training <- read.csv("https://raw.githubusercontent.com/andrewbowen19/data624Presentation/main/data/train.csv")
test <- read.csv("https://raw.githubusercontent.com/andrewbowen19/data624Presentation/main/data/test.csv")
```


We can take a brief look at our training data summary.
```{r train-summary}
summary(training)
```


We can take a look at the distribution of our outcome variable (`PH`)
```{r plot-ph-dist}
# Plot pH level distribution: our output variable
ggplot(training, aes(x=PH)) + geom_histogram()
```



### Data Wrangling
```{r wrangling}
# Convert Brand.Code to a factor as it's categorical
training$Brand.Code <- as.factor(training$Brand.Code)
test$Brand.Code <- as.factor(test$Brand.Code)
```


## Imputation
Our dataset is missing values, so we'll have to imput using the `mice` library in R. For our purposes, we'll use [predictive mean matching as our imputation method](https://stefvanbuuren.name/fimd/sec-pmm.html), since we may or may not need to transform our target variable this is a good method as it's robust to transformations of our output variable $Y$.
```{r message=FALSE, warning=FALSE}
# Impute using the mice package
# TODO(abowen): fix imputation issue
imputed <- mice(training, meth="pmm", seed=1234, printFlag = FALSE)
```


```{r}
# Set the training set to be imputed values
train <- complete(imputed)
head(train)
```

## Feature Selection

Next, we'll look at the correlations between our features and output PH

```{r}
correlations <- cor(as.matrix(train %>% dplyr::select(-c("Brand.Code"))))

corrplot(correlations)
```

We don't see a ton of strongly-correlated variables with our `PH` variable, so there's no strong candidates for a single feature to dominate the model. It's also helpful to see a pairwise distribution plot, which we can do via the `featurePlot` function from the `caret` library



## Modeling
We'll train four different types of models for this regression task:

- Multiple Linear Regression
- Random Forest Regressor
- XGBoost
- Neural Net

We'll predict on our test dataset (which we'll impute using the same method as the training dataset) using each of the models trained and write those to an output file for evaluation.

### Multiple Linear Regression
First, we'll fit a basic multiple linear regression model to our data. This can serve as a good "benchmark" model to compare our , as it's the simplest way to perform a regression task.
```{r linear-model}
# Fit the model against all features
full.model <- lm(PH ~., data = train)

# Next train a stepwise regression model based on the original
mlmModel <- stepAIC(full.model, direction = "both", trace = FALSE)
summary(mlmModel)
```


### Random Forest
We'll train a random forest to predict the `PH` variable as well. Random forests are an ensemble method based on training several decision trees

```{r}
# Train random forest mdoel
set.seed(12345)
rfModel <- randomForest(PH ~ ., data = train)
```


```{r}

plot(rfModel)
```

### Neural Network
We'll use the `nnet` package in R to train a neural net on our dataset.
```{r w}
myControl <- trainControl(## 3-fold CV
    method = "cv",
    number = 3)
  
nnGrid <-  expand.grid(size = seq(1, 10, 3),
                         decay = c(0, 0.2, 0.4))

# Train neural net model
nnModel <- train(PH~ .,
                   data = train,
                   method = "nnet",
                   maxit = 1000,
                   tuneGrid = nnGrid,
                   trainControl = myControl, trace = FALSE)
```

We'll plot our tuning loss as a function of our hyperparameter tuning
```{r}
plot(nnModel)
```


### XGBoost
One other poopular method is that of XGBoost. This is a boosting method using gradient descent to minimize the loss function

```{r}
# Set up parameter grid for XG Boost tuning
param <-  data.frame(nrounds=c(100), max_depth = c(1:10), eta=c(0.3), gamma=seq(0, 0.9, 0.1),
                     colsample_bytree=c(0.8), min_child_weight=c(1), subsample=c(1)) 

# Train XGBoost using Repeated crossfold validation
xgBoostModel <- train(PH~., data=train, method="xgbTree",
                      trControl=trainControl(method="repeatedcv"), tuneGrid=param)
```

Now we can plot our XGBoost model's hyperparameter tuning results
```{r}
plot(xgBoostModel)
```


## Model Evalutation


We'll need to impute our test data in order to not feed any `NaN` values into our models. We'll use the predictive mean matching method that we used above to impute our test values.
```{r impute-test-data}
testImputed <- mice(test, method="pmm", seed=1234, printFlag = FALSE)

test <- complete(testImputed)# %>% select(-c("PH"))#, "Brand.Code"))
```


Now we can use the four models we've trained 
```{r predict}
# Predict using all trained models
predictionsLinear <- predict(mlmModel, test)
predictionsRandomForest <- predict(rfModel, test)
predictionsNN <- predict(nnModel, test %>% dplyr::select(-c(PH)))
predictionsXGBoost <- predict(xgBoostModel, test)
```



Lastly, we'll combine our prediction columns into 
```{r combine-predictions}
predictions <- as.data.frame(cbind(linear = predictionsLinear,xgBoost = predictionsXGBoost, randomForest = predictionsRandomForest))#, neuralNet = predictionsNN)

# Replace with local path if needed
write_csv(predictions, "/Users/andrewbowen/CUNY/data624Presentation/data/project2-predictions.csv")
```

